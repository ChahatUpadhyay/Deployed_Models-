{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset sizes -> A: 2500, B: 2500. Steps/epoch: 833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params G: 14932227  Params D: 6962369\n",
      "Starting training loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1162376/1179768773.py:272: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler_G = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
      "/tmp/ipykernel_1162376/1179768773.py:273: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler_D_A = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
      "/tmp/ipykernel_1162376/1179768773.py:274: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler_D_B = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
      "/tmp/ipykernel_1162376/1179768773.py:292: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
      "/tmp/ipykernel_1162376/1179768773.py:325: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
      "/tmp/ipykernel_1162376/1179768773.py:343: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 317\u001b[0m\n\u001b[1;32m    313\u001b[0m     loss_cycle_B \u001b[38;5;241m=\u001b[39m criterion_cycle(recov_B, real_B) \u001b[38;5;241m*\u001b[39m lambda_cycle\n\u001b[1;32m    315\u001b[0m     loss_G \u001b[38;5;241m=\u001b[39m loss_GAN_A2B \u001b[38;5;241m+\u001b[39m loss_GAN_B2A \u001b[38;5;241m+\u001b[39m loss_cycle_A \u001b[38;5;241m+\u001b[39m loss_cycle_B \u001b[38;5;241m+\u001b[39m loss_id_A \u001b[38;5;241m+\u001b[39m loss_id_B\n\u001b[0;32m--> 317\u001b[0m \u001b[43mscaler_G\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_G\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m scaler_G\u001b[38;5;241m.\u001b[39mstep(optimizer_G)\n\u001b[1;32m    319\u001b[0m scaler_G\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# full_cyclegan_512.py\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import time\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------\n",
    "# CONFIG\n",
    "# -----------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# user config - change these paths/values accordingly\n",
    "img_size = 512\n",
    "batch_size = 3            # increase if you have VRAM\n",
    "epochs = 100\n",
    "save_interval = 5         # save every 5 epochs\n",
    "data_root = \"/teamspace/studios/this_studio/.lightning_studio/dataset\"    # must contain trainA/ and trainB/\n",
    "save_dir = \"/teamspace/studios/this_studio/.lightning_studio/checkpoints\"\n",
    "sample_dir = os.path.join(save_dir, \"samples\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "# optim params\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "# losses weights\n",
    "lambda_cycle = 10.0\n",
    "lambda_identity = 5.0\n",
    "\n",
    "# random seed (optional)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# -----------------------\n",
    "# DATASET\n",
    "# -----------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # maps [0,1] -> [-1,1]\n",
    "])\n",
    "\n",
    "class SingleImageFolder(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.files = [os.path.join(root_dir, f) for f in sorted(os.listdir(root_dir)) \n",
    "                      if f.lower().endswith((\"jpg\", \"jpeg\", \"png\"))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.files[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# paths for domain A (real photos) and domain B (Ghibli style)\n",
    "dataset_A = SingleImageFolder(os.path.join(data_root, \"trainA\"), transform=transform)\n",
    "dataset_B = SingleImageFolder(os.path.join(data_root, \"trainB\"), transform=transform)\n",
    "\n",
    "loader_A = DataLoader(dataset_A, batch_size=batch_size, shuffle=True, num_workers=6, drop_last=True)\n",
    "loader_B = DataLoader(dataset_B, batch_size=batch_size, shuffle=True, num_workers=6, drop_last=True)\n",
    "\n",
    "steps_per_epoch = min(len(loader_A), len(loader_B))\n",
    "print(f\"Dataset sizes -> A: {len(dataset_A)}, B: {len(dataset_B)}. Steps/epoch: {steps_per_epoch}\")\n",
    "\n",
    "# -----------------------\n",
    "# MODELS\n",
    "# -----------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=0),\n",
    "            nn.InstanceNorm2d(channels, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=0),\n",
    "            nn.InstanceNorm2d(channels, affine=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc=3, output_nc=3, n_residual_blocks=12):\n",
    "        super().__init__()\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=7, padding=0),\n",
    "            nn.InstanceNorm2d(64, affine=True),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        in_channels = 64\n",
    "        for _ in range(2):\n",
    "            out_channels = in_channels * 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_channels, affine=True),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_channels)]\n",
    "        for _ in range(2):\n",
    "            out_channels = in_channels // 2\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_channels, affine=True),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, output_nc, kernel_size=7),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        self.model = nn.Sequential(*model)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc=3):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        in_channels = 64\n",
    "        for _ in range(4):  # 4 additional => total 5 downsamples\n",
    "            out_channels = min(in_channels * 2, 512)\n",
    "            layers += [\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_channels, affine=True),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "        layers += [nn.Conv2d(in_channels, 1, kernel_size=4, padding=1)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# instantiate\n",
    "G_A2B = Generator(n_residual_blocks=12).to(device)\n",
    "G_B2A = Generator(n_residual_blocks=12).to(device)\n",
    "D_A = Discriminator().to(device)\n",
    "D_B = Discriminator().to(device)\n",
    "\n",
    "# -----------------------\n",
    "# weight init\n",
    "# -----------------------\n",
    "def init_weights(net, init_gain=0.02):\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            if getattr(m, \"bias\", None) is not None:\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif isinstance(m, (nn.InstanceNorm2d, nn.BatchNorm2d)):\n",
    "            if getattr(m, \"weight\", None) is not None:\n",
    "                nn.init.normal_(m.weight.data, 1.0, init_gain)\n",
    "            if getattr(m, \"bias\", None) is not None:\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "init_weights(G_A2B)\n",
    "init_weights(G_B2A)\n",
    "init_weights(D_A)\n",
    "init_weights(D_B)\n",
    "\n",
    "def count_params(m):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Params G:\", count_params(G_A2B), \" Params D:\", count_params(D_A))\n",
    "\n",
    "# -----------------------\n",
    "# Replay buffer\n",
    "# -----------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        assert max_size > 0\n",
    "        self.max_size = max_size\n",
    "        self.data = []  # list of tensors on CPU\n",
    "    def push_and_pop(self, data):\n",
    "        # data: tensor (B, C, H, W)\n",
    "        returned = []\n",
    "        for element in data.detach().cpu():\n",
    "            element = element.unsqueeze(0)  # (1,C,H,W)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element.clone())\n",
    "                returned.append(element.to(device))\n",
    "            else:\n",
    "                if random.uniform(0,1) > 0.5:\n",
    "                    idx = random.randint(0, self.max_size - 1)\n",
    "                    tmp = self.data[idx].clone().to(device)\n",
    "                    self.data[idx] = element.clone()\n",
    "                    returned.append(tmp)\n",
    "                else:\n",
    "                    returned.append(element.to(device))\n",
    "        return torch.cat(returned, dim=0)\n",
    "\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "# -----------------------\n",
    "# losses, optimizers, schedulers\n",
    "# -----------------------\n",
    "criterion_GAN = nn.MSELoss().to(device)\n",
    "criterion_cycle = nn.L1Loss().to(device)\n",
    "criterion_identity = nn.L1Loss().to(device)\n",
    "\n",
    "optimizer_G = optim.Adam(itertools.chain(G_A2B.parameters(), G_B2A.parameters()), lr=lr, betas=(beta1, beta2))\n",
    "optimizer_D_A = optim.Adam(D_A.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "optimizer_D_B = optim.Adam(D_B.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "decay_start_epoch = max(1, epochs // 2)\n",
    "def lambda_rule(epoch):\n",
    "    return 1.0 - max(0, epoch - decay_start_epoch) / float(max(1, epochs - decay_start_epoch))\n",
    "\n",
    "scheduler_G = optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambda_rule)\n",
    "scheduler_D_A = optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=lambda_rule)\n",
    "scheduler_D_B = optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=lambda_rule)\n",
    "\n",
    "# -----------------------\n",
    "# utilities: sampling images for visualization\n",
    "# -----------------------\n",
    "def denorm(tensor):\n",
    "    # tensor in [-1,1] -> [0,1]\n",
    "    return (tensor + 1.0) / 2.0\n",
    "\n",
    "def sample_images(epoch, G_A2B, G_B2A, loader_for_sampling, n_samples=4):\n",
    "    G_A2B.eval(); G_B2A.eval()\n",
    "    try:\n",
    "        real_A = next(iter(loader_for_sampling[0]))\n",
    "        real_B = next(iter(loader_for_sampling[1]))\n",
    "    except Exception:\n",
    "        # fallback: get first batch from each loader\n",
    "        real_A = next(iter(loader_A))\n",
    "        real_B = next(iter(loader_B))\n",
    "    real_A = real_A[:n_samples].to(device)\n",
    "    real_B = real_B[:n_samples].to(device)\n",
    "    with torch.no_grad():\n",
    "        fake_B = G_A2B(real_A)\n",
    "        fake_A = G_B2A(real_B)\n",
    "        rec_A = G_B2A(fake_B)\n",
    "        rec_B = G_A2B(fake_A)\n",
    "    # build grid: top rows A_real, A_fake, A_rec ; bottom rows B_real, B_fake, B_rec\n",
    "    grid = torch.cat([real_A, fake_B, rec_A, real_B, fake_A, rec_B], dim=0)\n",
    "    grid = denorm(grid)\n",
    "    grid = vutils.make_grid(grid, nrow=n_samples, padding=2)\n",
    "    path = os.path.join(sample_dir, f\"epoch_{epoch}.png\")\n",
    "    vutils.save_image(grid, path)\n",
    "    print(f\"Saved sample image to: {path}\")\n",
    "    G_A2B.train(); G_B2A.train()\n",
    "\n",
    "# -----------------------\n",
    "# Mixed precision scaler\n",
    "# -----------------------\n",
    "use_amp = True if torch.cuda.is_available() else False\n",
    "scaler_G = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "scaler_D_A = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "scaler_D_B = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "# -----------------------\n",
    "# Training loop\n",
    "# -----------------------\n",
    "print(\"Starting training loop...\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start = time.time()\n",
    "    # iterate zipped loaders (stops at smaller loader length)\n",
    "    loop = zip(loader_A, loader_B)\n",
    "    for i, (real_A, real_B) in enumerate(loop, start=1):\n",
    "        real_A = real_A.to(device)\n",
    "        real_B = real_B.to(device)\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # Train Generators (G_A2B & G_B2A)\n",
    "        # -----------------------------------------\n",
    "        optimizer_G.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            # identity\n",
    "            idt_B = G_A2B(real_B)\n",
    "            loss_id_B = criterion_identity(idt_B, real_B) * lambda_identity\n",
    "            idt_A = G_B2A(real_A)\n",
    "            loss_id_A = criterion_identity(idt_A, real_A) * lambda_identity\n",
    "\n",
    "            # GAN loss\n",
    "            fake_B = G_A2B(real_A)\n",
    "            pred_fake_B = D_B(fake_B)\n",
    "            loss_GAN_A2B = criterion_GAN(pred_fake_B, torch.ones_like(pred_fake_B))\n",
    "\n",
    "            fake_A = G_B2A(real_B)\n",
    "            pred_fake_A = D_A(fake_A)\n",
    "            loss_GAN_B2A = criterion_GAN(pred_fake_A, torch.ones_like(pred_fake_A))\n",
    "\n",
    "            # cycle loss\n",
    "            recov_A = G_B2A(fake_B)\n",
    "            loss_cycle_A = criterion_cycle(recov_A, real_A) * lambda_cycle\n",
    "\n",
    "            recov_B = G_A2B(fake_A)\n",
    "            loss_cycle_B = criterion_cycle(recov_B, real_B) * lambda_cycle\n",
    "\n",
    "            loss_G = loss_GAN_A2B + loss_GAN_B2A + loss_cycle_A + loss_cycle_B + loss_id_A + loss_id_B\n",
    "\n",
    "        scaler_G.scale(loss_G).backward()\n",
    "        scaler_G.step(optimizer_G)\n",
    "        scaler_G.update()\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # Train Discriminator A\n",
    "        # -----------------------------------------\n",
    "        optimizer_D_A.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            pred_real = D_A(real_A)\n",
    "            loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "            fake_A_for_disc = fake_A_buffer.push_and_pop(fake_A.detach())  # (B,C,H,W)\n",
    "            pred_fake = D_A(fake_A_for_disc)\n",
    "            loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "            loss_D_A = (loss_D_real + loss_D_fake) * 0.5\n",
    "\n",
    "        scaler_D_A.scale(loss_D_A).backward()\n",
    "        scaler_D_A.step(optimizer_D_A)\n",
    "        scaler_D_A.update()\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # Train Discriminator B\n",
    "        # -----------------------------------------\n",
    "        optimizer_D_B.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            pred_real = D_B(real_B)\n",
    "            loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "            fake_B_for_disc = fake_B_buffer.push_and_pop(fake_B.detach())\n",
    "            pred_fake = D_B(fake_B_for_disc)\n",
    "            loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "            loss_D_B = (loss_D_real + loss_D_fake) * 0.5\n",
    "\n",
    "        scaler_D_B.scale(loss_D_B).backward()\n",
    "        scaler_D_B.step(optimizer_D_B)\n",
    "        scaler_D_B.update()\n",
    "\n",
    "        # Logging (every batch)\n",
    "        if i % 10 == 0 or i == steps_per_epoch:\n",
    "            print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{steps_per_epoch}] \"\n",
    "                  f\"[G {loss_G.item():.4f}] [D_A {loss_D_A.item():.4f}] [D_B {loss_D_B.item():.4f}]\")\n",
    "\n",
    "        # limit to steps_per_epoch\n",
    "        if i >= steps_per_epoch:\n",
    "            break\n",
    "\n",
    "    # update schedulers\n",
    "    scheduler_G.step()\n",
    "    scheduler_D_A.step()\n",
    "    scheduler_D_B.step()\n",
    "\n",
    "    # save checkpoints every save_interval epochs\n",
    "    if epoch % save_interval == 0 or epoch == 1:\n",
    "        ckpt = {\n",
    "            \"epoch\": epoch,\n",
    "            \"G_A2B\": G_A2B.state_dict(),\n",
    "            \"G_B2A\": G_B2A.state_dict(),\n",
    "            \"D_A\": D_A.state_dict(),\n",
    "            \"D_B\": D_B.state_dict(),\n",
    "            \"optim_G\": optimizer_G.state_dict(),\n",
    "            \"optim_D_A\": optimizer_D_A.state_dict(),\n",
    "            \"optim_D_B\": optimizer_D_B.state_dict()\n",
    "        }\n",
    "        torch.save(ckpt, os.path.join(save_dir, f\"cyclegan_epoch_{epoch}.pth\"))\n",
    "        print(f\"Saved checkpoint epoch {epoch} -> {os.path.join(save_dir, f'cyclegan_epoch_{epoch}.pth')}\")\n",
    "\n",
    "    # sample images for qualitative check\n",
    "    sample_images(epoch, G_A2B, G_B2A, loader_for_sampling=(loader_A, loader_B), n_samples=min(4, batch_size))\n",
    "\n",
    "    print(f\"Epoch {epoch} completed in {time.time() - epoch_start:.2f}s\")\n",
    "\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint keys: ['epoch', 'G_A2B', 'G_B2A', 'D_A', 'D_B', 'optim_G', 'optim_D_A', 'optim_D_B']\n",
      "Loaded weights from ckpt['G_A2B'].\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 't' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 121\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    119\u001b[0m     fake \u001b[38;5;241m=\u001b[39m G(input_tensor)\n\u001b[0;32m--> 121\u001b[0m out_pil \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_to_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m out_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(test_image_path)\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ghibli.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m out_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_dir, out_name)\n",
      "Cell \u001b[0;32mIn[11], line 111\u001b[0m, in \u001b[0;36mtensor_to_pil\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtensor_to_pil\u001b[39m(tensor):\n\u001b[0;32m--> 111\u001b[0m     t \u001b[38;5;241m=\u001b[39m denorm(\u001b[43mt\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transforms\u001b[38;5;241m.\u001b[39mToPILImage()(t)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 't' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# standalone_test_ghibli.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------- CONFIG --------\n",
    "checkpoint_path = \"/teamspace/studios/this_studio/.lightning_studio/checkpoints/cyclegan_epoch_85.pth\"\n",
    "test_image_path = \"/teamspace/studios/this_studio/.lightning_studio/test_5.jpg\"\n",
    "out_dir = \"/teamspace/studios/this_studio/.lightning_studio/Outputs\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# -------- Generator class (must match training) --------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=0),\n",
    "            nn.InstanceNorm2d(channels, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=0),\n",
    "            nn.InstanceNorm2d(channels, affine=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc=3, output_nc=3, n_residual_blocks=12):\n",
    "        super().__init__()\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=7, padding=0),\n",
    "            nn.InstanceNorm2d(64, affine=True),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        in_channels = 64\n",
    "        for _ in range(2):\n",
    "            out_channels = in_channels * 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_channels, affine=True),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_channels)]\n",
    "\n",
    "        for _ in range(2):\n",
    "            out_channels = in_channels // 2\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_channels, affine=True),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, output_nc, kernel_size=7),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# -------- Instantiate generator and load checkpoint --------\n",
    "G = Generator(n_residual_blocks=12).to(device)\n",
    "\n",
    "ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "# print keys for debugging if needed\n",
    "if isinstance(ckpt, dict):\n",
    "    print(\"Checkpoint keys:\", list(ckpt.keys()))\n",
    "else:\n",
    "    print(\"Checkpoint is not a dict (it's probably a state_dict).\")\n",
    "\n",
    "# Load weights: preferred path if you saved dict with \"G_A2B\"\n",
    "if isinstance(ckpt, dict) and \"G_A2B\" in ckpt:\n",
    "    G.load_state_dict(ckpt[\"G_A2B\"])\n",
    "    print(\"Loaded weights from ckpt['G_A2B'].\")\n",
    "else:\n",
    "    # fallback: try loading the whole checkpoint as a state_dict\n",
    "    try:\n",
    "        G.load_state_dict(ckpt)\n",
    "        print(\"Loaded checkpoint as state_dict directly.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Couldn't load generator weights from checkpoint. Error: {e}\")\n",
    "\n",
    "G.eval()\n",
    "\n",
    "# -------- Preprocess / postprocess --------\n",
    "img_size = 512\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
    "])\n",
    "\n",
    "def denorm(tensor):\n",
    "    return (tensor.clamp(-1,1) + 1.0) / 2.0\n",
    "\n",
    "def tensor_to_pil(tensor):\n",
    "    t = denorm(t.squeeze(0)).cpu()\n",
    "    return transforms.ToPILImage()(t)\n",
    "\n",
    "# -------- Inference --------\n",
    "img = Image.open(test_image_path).convert(\"RGB\")\n",
    "input_tensor = preprocess(img).unsqueeze(0).to(device)  # shape (1,C,H,W)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fake = G(input_tensor)\n",
    "\n",
    "out_pil = tensor_to_pil(fake)\n",
    "out_name = os.path.basename(test_image_path).rsplit(\".\",1)[0] + \"_ghibli.png\"\n",
    "out_path = os.path.join(out_dir, out_name)\n",
    "out_pil.save(out_path)\n",
    "print(\"Saved output to:\", out_path)\n",
    "\n",
    "# show side-by-side\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1); plt.title(\"Original\"); plt.axis(\"off\"); plt.imshow(img)\n",
    "plt.subplot(1,2,2); plt.title(\"Ghibli Style\"); plt.axis(\"off\"); plt.imshow(out_pil)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
